{% extends "blogpost.html" %}

{% block title %}
<h1>Panopticon</h1>
{% endblock %}

{% block date %}
<p class="blogpost__date">Published on April 22, 2021</p>
{% endblock %}

{% block image %}
<img
  src="{{ url_for('static', filename='img/panopticon.png') }}"
  alt="Panopticon plan"
  class="blogpost__image" />
{% endblock %}

{% block article %}
<p>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
Social theorist Jeremy Bentham’s original projection for a
panoptic prison, made it so a prisoner never knew if he was being
watched. A circular prison with a single guard tower in the middle
was the original blueprint for the structure.</p>
<p>It seems we have created some heterarchical form of panopticon
on the internet, where just about everybody is spying on
everybody else from behind sunglasses and masks. In particular
big data corporations and nation states seem very busy with
that, in very organised and automated ways. Hack-all-around-you,
surveillance, and <a
href="https://tymyrddin.github.io/da-threat-model/">de-anonymisation</a>
seem to have become common practice (and competition).</p>
<ul>
<li><a href="https://gitlab.com/tymyrddin">Machine learning (ML)</a> is a key component in technologies
such as predictive analytics and artificial intelligence. The
Computer seems to be running the show.</li>
<li>Most of the current AI offerings on the market can identify
associations in large quantities of data, but can not work out
complex phenomena of cause and effect or identify modifiable
factors that can engender desired outcomes. Yet. I don’t know
what is more dangerous, a world run by <a
href="https://www.businessinsider.fr/us/what-to-know-about-blackrock-larry-fink-biden-cabinet-facts-2020-12">Black
Rock</a>’s <a
href="https://en.wikipedia.org/wiki/Aladdin_(BlackRock)">Aladdin</a>-like
    machines as if they can, or development of machines that
actually can and then making ourselves dependent on these even
more.</li>
<li>ML is used to review large volumes of data and identify
patterns and trends. An error, for example a faulty sensor
generating a flawed data set, can cause rippling effects
within a machine learning interface, because events after the error may be flawed, skewed or just plain undesirable.
Errors do occur, and it is a susceptibility that developers
have thus far been unable to premeditate and negate
consistently. Imagine, if people were to rely on these
patterns and trends?!?</li>
</ul>
{% endblock %}
